{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import lit,to_timestamp, coalesce, col, date_format, when\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "import pymysql\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define la ruta base\n",
    "base_path = Path(\"/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS\")\n",
    "\n",
    "# Lista 1: Archivos .csv que contienen \"AQ_SEP\" en su nombre\n",
    "lista_aq_sep = [archivo for archivo in base_path.rglob(\"*.csv\") if \"AQ_SEP\" in archivo.name]\n",
    "\n",
    "# Lista 2: Archivos .csv que contienen \"AirQualityUnit01\" en su nombre\n",
    "lista_airqualityunit01 = [archivo for archivo in base_path.rglob(\"*.csv\") if \"AirQualityUnit01\" in archivo.name]\n",
    "\n",
    "# Lista 3: Archivos .csv que contienen \"AirQualityUnit02\" en su nombre\n",
    "lista_airqualityunit02 = [archivo for archivo in base_path.rglob(\"*.csv\") if \"AirQualityUnit02\" in archivo.name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos con 'AQ_SEP':\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Abril 2024/6etmrenvironmentobserved - AQ_SEP-data-2024-04-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Junio 2024/6etmrenvironmentobserved - AQ_SEP, activa desde el 1ero de mayo-data-2024-06-14.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Agosto 2024/6etmrenvironmentobserved - AQ_SEP, activa desde el 1ero de mayo-data-2024-08-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Febrero 2024/6etmrenvironmentobserved - AQ_SEP-data-2024-02-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Diciembre 2023/6etmrenvironmentobserved - AQ_SEP-data-2023-12-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Marzo 2024/6etmrenvironmentobserved - AQ_SEP-data-2024-03-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Noviembre 2023/6etmrenvironmentobserved - AQ_SEP-data-2023-11-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Octubre 2023/6etmrenvironmentobserved - AQ_SEP-data-2023-10-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Enero 2024/6etmrenvironmentobserved - AQ_SEP-data-2024-01-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Agosto 2023/6-etmrenvironmentobserved - AQ_SEP-data-2023-08-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Septiembre 2023/6etmrenvironmentobserved - AQ_SEP-data-2023-09-14.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Mayo 2024/6etmrenvironmentobserved - AQ_SEP-data-2024-05-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Julio 2023/6-etmrenvironmentobserved-AQ_SEP-01.csv\n"
     ]
    }
   ],
   "source": [
    "# Imprimir las listas archivos AQ_SEP\n",
    "print(\"Archivos con 'AQ_SEP':\")\n",
    "for archivo in lista_aq_sep:\n",
    "    print(archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Archivos con 'AirQualityUnit01':\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Abril 2024/7etmrairqualityobserved - AirQualityUnit01-data-2024-04-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Junio 2024/7etmrairqualityobserved - AirQualityUnit01-data-2024-06-14.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Julio 2024/7etmrairqualityobserved - AirQualityUnit01-data-2024-07-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Agosto 2024/7etmrairqualityobserved - AirQualityUnit01-data-2024-08-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Febrero 2024/7etmrairqualityobserved - AirQualityUnit01-data-2024-02-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Diciembre 2023/7etmrairqualityobserved - AirQualityUnit01-data-2023-12-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Marzo 2024/7etmrairqualityobserved - AirQualityUnit01-data-2024-03-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Noviembre 2023/7etmrairqualityobserved - AirQualityUnit01-data-2023-11-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Octubre 2023/7etmrairqualityobserved - AirQualityUnit01-data-2023-10-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Enero 2024/7etmrairqualityobserved - AirQualityUnit01-data-2024-01-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Agosto 2023/7-etmrairqualityobserved - AirQualityUnit01-data-2023-08-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Septiembre 2023/7etmrairqualityobserved - AirQualityUnit01-data-2023-09-14.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Mayo 2024/7etmrairqualityobserved - AirQualityUnit01-data-2024-05-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Julio 2023/7-etmrairqualityobserved-AirQualityUnit01-01.csv\n"
     ]
    }
   ],
   "source": [
    "# Imprimir las listas archivos AirQualityUnit01\n",
    "print(\"\\nArchivos con 'AirQualityUnit01':\")\n",
    "for archivo in lista_airqualityunit01:\n",
    "    print(archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Archivos con 'AirQualityUnit02':\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Abril 2024/8etmrairqualityobserved - AirQualityUnit02-data-2024-04-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Junio 2024/8etmrairqualityobserved - AirQualityUnit02-data-2024-06-14.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Agosto 2024/8etmrairqualityobserved - AirQualityUnit02-data-2024-08-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Febrero 2024/8etmrairqualityobserved - AirQualityUnit02-data-2024-02-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Diciembre 2023/8etmrairqualityobserved - AirQualityUnit02-data-2023-12-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Marzo 2024/8etmrairqualityobserved - AirQualityUnit02-data-2024-03-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Noviembre 2023/8etmrairqualityobserved - AirQualityUnit02-data-2023-11-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Octubre 2023/8etmrairqualityobserved - AirQualityUnit02-data-2023-10-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Enero 2024/8etmrairqualityobserved - AirQualityUnit02-data-2024-01-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Agosto 2023/8-etmrairqualityobserved - AirQualityUnit02-data-2023-08-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Septiembre 2023/8etmrairqualityobserved - AirQualityUnit02-data-2023-09-14.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Mayo 2024/8etmrairqualityobserved - AirQualityUnit02-data-2024-05-15.csv\n",
      "/home/steven-laptop/Documentos/Estudio/Proyecto QAIR/datos/DATOS/Julio 2023/8-etmrairqualityobserved-AirQualityUnit02-01.csv\n"
     ]
    }
   ],
   "source": [
    "# Imprimir las listas archivos AirQualityUnit02\n",
    "print(\"\\nArchivos con 'AirQualityUnit02':\")\n",
    "for archivo in lista_airqualityunit02:\n",
    "    print(archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_separator_simple(file_path, sample_size=1024):\n",
    "    \"\"\"\n",
    "    Detecta el separador de un archivo CSV leyendo una muestra del archivo.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Ruta del archivo CSV.\n",
    "        sample_size (int, optional): Número de bytes a leer para el análisis. Por defecto es 1024.\n",
    "\n",
    "    Returns:\n",
    "        str: El caracter delimitador detectado o None si no se pudo detectar.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            sample = f.read(sample_size)\n",
    "        dialect = csv.Sniffer().sniff(sample)\n",
    "        return dialect.delimiter\n",
    "    except Exception as e:\n",
    "        print(f\"Error al detectar el separador: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_correctly(path):\n",
    "    # Leer el archivo como texto\n",
    "    raw_df = spark.read.text(path)\n",
    "    # Obtener la primera línea\n",
    "    first_line = raw_df.first()[0]\n",
    "    separator = detect_separator_simple(path)\n",
    "    \n",
    "    if first_line and isinstance(first_line, str) and first_line.startswith(\"sep=\"):\n",
    "        # Omitir la primera línea usando zipWithIndex para mantener el orden\n",
    "        lines_rdd = raw_df.rdd.zipWithIndex() \\\n",
    "                        .filter(lambda row_index: row_index[1] > 0) \\\n",
    "                        .map(lambda row_index: row_index[0][0])\n",
    "        \n",
    "        # Leer el CSV a partir del RDD de líneas\n",
    "        df = spark.read.option(\"header\", \"true\") \\\n",
    "                       .option(\"inferSchema\", \"true\") \\\n",
    "                       .option(\"escape\", '\"') \\\n",
    "                       .option(\"quote\", '\"') \\\n",
    "                       .option(\"sep\", separator) \\\n",
    "                       .csv(lines_rdd)\n",
    "    else:\n",
    "        # Leer el CSV normalmente desde la ruta\n",
    "        df = spark.read.option(\"header\", \"true\") \\\n",
    "                       .option(\"inferSchema\", \"true\") \\\n",
    "                       .option(\"escape\", '\"') \\\n",
    "                       .option(\"quote\", '\"') \\\n",
    "                       .option(\"sep\", separator) \\\n",
    "                       .csv(path)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_LIST_AQ_SEP = [load_csv_correctly(str(path)) for path in lista_aq_sep]\n",
    "DF_LIST_AQ_01 = [load_csv_correctly(str(path)) for path in lista_airqualityunit01]\n",
    "DF_LIST_AQ_02 = [load_csv_correctly(str(path)) for path in lista_airqualityunit02]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para limpiar los nombres de columnas\n",
    "def clean_column_names(df):\n",
    "    new_columns = [col.strip().replace(\" \", \"_\").lower() for col in df.columns]\n",
    "    return df.toDF(*new_columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_LIST_AQ_SEP = [clean_column_names(df) for df in DF_LIST_AQ_SEP]\n",
    "DF_LIST_AQ_01 = [clean_column_names(df) for df in DF_LIST_AQ_01]\n",
    "DF_LIST_AQ_02 = [clean_column_names(df) for df in DF_LIST_AQ_02]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame DF_LIST_AQ_SEP 0 columns: ['time', 'co_aq_sep', 'no2_aq_sep', 'o3_aq_sep', 'pm1_aq_sep', 'pm10_aq_sep', 'pm2_5_aq_sep', 'pressure_aq_sep', 'relativehumidity_aq_sep', 'temperature_aq_sep']\n",
      "DataFrame DF_LIST_AQ_SEP 1 columns: ['time', 'co_aq_sep', 'humidity_aq_sep', 'o3_aq_sep', 'pm1_aq_sep', 'pm10_aq_sep', 'pm2_5_aq_sep', 'pressure_aq_sep', 'temperature_aq_sep']\n",
      "DataFrame DF_LIST_AQ_SEP 2 columns: ['time', 'co_aq_sep', 'humidity_aq_sep', 'o3_aq_sep', 'pm1_aq_sep', 'pm10_aq_sep', 'pm2_5_aq_sep', 'pressure_aq_sep', 'temperature_aq_sep']\n",
      "DataFrame DF_LIST_AQ_SEP 3 columns: ['time', 'co_aq_sep', 'no2_aq_sep', 'o3_aq_sep', 'pm1_aq_sep', 'pm10_aq_sep', 'pm2_5_aq_sep', 'pressure_aq_sep', 'relativehumidity_aq_sep', 'temperature_aq_sep']\n",
      "DataFrame DF_LIST_AQ_SEP 4 columns: ['time', 'co_aq_sep', 'no2_aq_sep', 'o3_aq_sep', 'pm1_aq_sep', 'pm10_aq_sep', 'pm2_5_aq_sep', 'pressure_aq_sep', 'relativehumidity_aq_sep', 'temperature_aq_sep']\n",
      "DataFrame DF_LIST_AQ_SEP 5 columns: ['time', 'co_aq_sep', 'no2_aq_sep', 'o3_aq_sep', 'pm1_aq_sep', 'pm10_aq_sep', 'pm2_5_aq_sep', 'pressure_aq_sep', 'relativehumidity_aq_sep', 'temperature_aq_sep']\n",
      "DataFrame DF_LIST_AQ_SEP 6 columns: ['time', 'co_aq_sep', 'no2_aq_sep', 'o3_aq_sep', 'pm1_aq_sep', 'pm10_aq_sep', 'pm2_5_aq_sep', 'pressure_aq_sep', 'relativehumidity_aq_sep', 'temperature_aq_sep']\n",
      "DataFrame DF_LIST_AQ_SEP 7 columns: ['time', 'co_aq_sep', 'no2_aq_sep', 'o3_aq_sep', 'pm1_aq_sep', 'pm10_aq_sep', 'pm2_5_aq_sep', 'pressure_aq_sep', 'relativehumidity_aq_sep', 'temperature_aq_sep']\n",
      "DataFrame DF_LIST_AQ_SEP 8 columns: ['time', 'co_aq_sep', 'no2_aq_sep', 'o3_aq_sep', 'pm1_aq_sep', 'pm10_aq_sep', 'pm2_5_aq_sep', 'pressure_aq_sep', 'relativehumidity_aq_sep', 'temperature_aq_sep']\n",
      "DataFrame DF_LIST_AQ_SEP 9 columns: ['time', 'address', 'battery', 'co', 'dateobserved', 'entity_id', 'entity_type', 'fiware_servicepath', 'location', 'location_centroid', 'no2', 'o3', 'pm1', 'pm10', 'pm2_5', 'pressure', 'relativehumidity', 'temperature', 'time_index']\n",
      "DataFrame DF_LIST_AQ_SEP 10 columns: ['time', 'entity_id', 'co', 'no2', 'o3', 'pm1', 'pm10', 'pm2_5', 'pressure', 'relativehumidity', 'temperature']\n",
      "DataFrame DF_LIST_AQ_SEP 11 columns: ['time', 'co_aq_sep', 'no2_aq_sep', 'o3_aq_sep', 'pm1_aq_sep', 'pm10_aq_sep', 'pm2_5_aq_sep', 'pressure_aq_sep', 'relativehumidity_aq_sep', 'temperature_aq_sep']\n",
      "DataFrame DF_LIST_AQ_SEP 12 columns: ['time', 'address', 'battery', 'co', 'dateobserved', 'entity_id', 'entity_type', 'fiware_servicepath', 'location', 'location_centroid', 'no2', 'o3', 'pm1', 'pm10', 'pm2_5', 'pressure', 'relativehumidity', 'temperature', 'time_index']\n"
     ]
    }
   ],
   "source": [
    "for i, df in enumerate(DF_LIST_AQ_SEP):\n",
    "    print(f\"DataFrame DF_LIST_AQ_SEP {i} columns: {df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame DF_LIST_AQ_01 0 columns: ['time', 'entity_id', 'co', 'co2', 'o3', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'temperature']\n",
      "DataFrame DF_LIST_AQ_01 1 columns: ['time', 'entity_id', 'co', 'co2', 'o3', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'temperature']\n",
      "DataFrame DF_LIST_AQ_01 2 columns: ['time', 'entity_id', 'co', 'co2', 'o3', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'temperature']\n",
      "DataFrame DF_LIST_AQ_01 3 columns: ['time', 'entity_id', 'co', 'co2', 'o3', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'temperature']\n",
      "DataFrame DF_LIST_AQ_01 4 columns: ['time', 'entity_id', 'co', 'co2', 'o3', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'temperature']\n",
      "DataFrame DF_LIST_AQ_01 5 columns: ['time', 'entity_id', 'co', 'co2', 'o3', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'temperature']\n",
      "DataFrame DF_LIST_AQ_01 6 columns: ['time', 'entity_id', 'co', 'co2', 'o3', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'temperature']\n",
      "DataFrame DF_LIST_AQ_01 7 columns: ['time', 'entity_id', 'co', 'co2', 'o3', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'temperature']\n",
      "DataFrame DF_LIST_AQ_01 8 columns: ['time', 'co_airqualityunit01', 'co2_airqualityunit01', 'o3_airqualityunit01', 'pm10_airqualityunit01', 'pm2_5_airqualityunit01', 'pm5_airqualityunit01', 'relativehumidity_airqualityunit01', 'so2_airqualityunit01', 'temperature_airqualityunit01']\n",
      "DataFrame DF_LIST_AQ_01 9 columns: ['time', 'entity_id', 'co', 'co2', 'o3', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'temperature']\n",
      "DataFrame DF_LIST_AQ_01 10 columns: ['time', 'address', 'co', 'co2', 'dateobserved', 'entity_id', 'entity_type', 'fiware_servicepath', 'latitude', 'location', 'location_centroid', 'longitude', 'no2', 'o3', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'source', 'temperature', 'time_index']\n",
      "DataFrame DF_LIST_AQ_01 11 columns: ['time', 'entity_id', 'co2', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'temperature']\n",
      "DataFrame DF_LIST_AQ_01 12 columns: ['time', 'entity_id', 'co', 'co2', 'o3', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'temperature']\n",
      "DataFrame DF_LIST_AQ_01 13 columns: ['time', 'address', 'co', 'co2', 'dateobserved', 'entity_id', 'entity_type', 'fiware_servicepath', 'latitude', 'location', 'location_centroid', 'longitude', 'no2', 'o3', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'source', 'temperature', 'time_index']\n"
     ]
    }
   ],
   "source": [
    "for i, df in enumerate(DF_LIST_AQ_01):\n",
    "    print(f\"DataFrame DF_LIST_AQ_01 {i} columns: {df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame DF_LIST_AQ_02 0 columns: ['time', 'entity_id', 'co', 'co2', 'o3', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'temperature']\n",
      "DataFrame DF_LIST_AQ_02 1 columns: ['time', 'entity_id', 'co', 'co2', 'o3', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'temperature']\n",
      "DataFrame DF_LIST_AQ_02 2 columns: ['time', 'entity_id', 'co', 'co2', 'o3', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'temperature']\n",
      "DataFrame DF_LIST_AQ_02 3 columns: ['time', 'entity_id', 'co', 'co2', 'o3', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'temperature']\n",
      "DataFrame DF_LIST_AQ_02 4 columns: ['time', 'entity_id', 'co', 'co2', 'o3', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'temperature']\n",
      "DataFrame DF_LIST_AQ_02 5 columns: ['time', 'entity_id', 'co', 'co2', 'o3', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'temperature']\n",
      "DataFrame DF_LIST_AQ_02 6 columns: ['time', 'entity_id', 'co', 'co2', 'o3', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'temperature']\n",
      "DataFrame DF_LIST_AQ_02 7 columns: ['time', 'co_airqualityunit02', 'co2_airqualityunit02', 'o3_airqualityunit02', 'pm10_airqualityunit02', 'pm2_5_airqualityunit02', 'pm5_airqualityunit02', 'relativehumidity_airqualityunit02', 'so2_airqualityunit02', 'temperature_airqualityunit02']\n",
      "DataFrame DF_LIST_AQ_02 8 columns: ['time', 'entity_id', 'co', 'co2', 'o3', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'temperature']\n",
      "DataFrame DF_LIST_AQ_02 9 columns: ['time', 'address', 'co', 'co2', 'dateobserved', 'entity_id', 'entity_type', 'fiware_servicepath', 'latitude', 'location', 'location_centroid', 'longitude', 'no2', 'o3', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'source', 'temperature', 'time_index']\n",
      "DataFrame DF_LIST_AQ_02 10 columns: ['time', 'entity_id', 'co', 'co2', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'temperature']\n",
      "DataFrame DF_LIST_AQ_02 11 columns: ['time', 'entity_id', 'co', 'co2', 'o3', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'temperature']\n",
      "DataFrame DF_LIST_AQ_02 12 columns: ['time', 'address', 'co', 'co2', 'dateobserved', 'entity_id', 'entity_type', 'fiware_servicepath', 'latitude', 'location', 'location_centroid', 'longitude', 'no2', 'o3', 'pm10', 'pm2_5', 'pm5', 'relativehumidity', 'so2', 'source', 'temperature', 'time_index']\n"
     ]
    }
   ],
   "source": [
    "for i, df in enumerate(DF_LIST_AQ_02):\n",
    "    print(f\"DataFrame DF_LIST_AQ_02 {i} columns: {df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_df(df):\n",
    "    # Lista de columnas deseadas en el orden y con los nombres estándar\n",
    "    desired_order = [\"time\", \"pm2_5\", \"pm10\", \"co\", \"o3\", \"humidity\", \"temperature\",\"pm5\"]\n",
    "    new_columns = []\n",
    "    \n",
    "    # Para cada columna deseada, buscar en el DataFrame original la primera columna que contenga la palabra clave\n",
    "    for keyword in desired_order:\n",
    "        # Buscar candidates: columnas cuyo nombre (en minúscula) contenga la palabra clave\n",
    "        candidates = [col for col in df.columns if keyword in col.lower()]\n",
    "        if candidates:\n",
    "            # Si existen varias, se toma la primera (podrías ajustar esto si necesitas otro criterio)\n",
    "            new_columns.append(F.col(candidates[0]).alias(keyword))\n",
    "        else:\n",
    "            # Si no existe la columna, se agrega una columna con valores nulos\n",
    "            new_columns.append(F.lit(None).alias(keyword))\n",
    "    \n",
    "    # Seleccionar y devolver el nuevo DataFrame con las columnas en el orden deseado\n",
    "    return df.select(*new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame DF_LIST_AQ_SEP_S 0 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 2927\n",
      "DataFrame DF_LIST_AQ_SEP_S 1 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 1525\n",
      "DataFrame DF_LIST_AQ_SEP_S 2 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 1591\n",
      "DataFrame DF_LIST_AQ_SEP_S 3 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 2866\n",
      "DataFrame DF_LIST_AQ_SEP_S 4 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 2765\n",
      "DataFrame DF_LIST_AQ_SEP_S 5 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 2657\n",
      "DataFrame DF_LIST_AQ_SEP_S 6 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 2959\n",
      "DataFrame DF_LIST_AQ_SEP_S 7 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 2859\n",
      "DataFrame DF_LIST_AQ_SEP_S 8 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 2800\n",
      "DataFrame DF_LIST_AQ_SEP_S 9 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 2766\n",
      "DataFrame DF_LIST_AQ_SEP_S 10 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 2824\n",
      "DataFrame DF_LIST_AQ_SEP_S 11 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 1598\n",
      "DataFrame DF_LIST_AQ_SEP_S 12 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 2035\n",
      "Total de filas en todos los DataFrames: 32172\n",
      "+-------------------+-----+----+----+---+--------+-----------+----+\n",
      "|               time|pm2_5|pm10|  co| o3|humidity|temperature| pm5|\n",
      "+-------------------+-----+----+----+---+--------+-----------+----+\n",
      "|2024-03-15 00:11:34|  0.5| 0.5|25.3|0.1|    -999|       11.5|NULL|\n",
      "|2024-03-15 00:26:34|  0.7| 0.7|23.0|0.1|    -999|       11.6|NULL|\n",
      "|2024-03-15 00:41:34|  0.7| 0.7|22.2|0.1|    -999|       11.2|NULL|\n",
      "|2024-03-15 00:56:34|  0.6| 0.6|20.4|0.1|    -999|       11.1|NULL|\n",
      "|2024-03-15 01:11:34|  0.6| 0.6|19.6|0.1|    -999|       10.6|NULL|\n",
      "|2024-03-15 01:26:34|  0.5| 0.5|19.2|0.1|    -999|       10.6|NULL|\n",
      "|2024-03-15 01:41:34|  0.7| 0.7|19.5|0.1|    -999|       10.4|NULL|\n",
      "|2024-03-15 01:56:34|  0.6| 0.6|21.2|0.1|    -999|       10.5|NULL|\n",
      "|2024-03-15 02:11:34|  0.5| 0.5|20.3|0.1|    -999|       10.1|NULL|\n",
      "|2024-03-15 02:26:34|  0.6| 0.6|19.2|0.1|    -999|        9.9|NULL|\n",
      "|2024-03-15 02:41:34|  0.5| 0.5|19.7|0.1|    -999|        9.7|NULL|\n",
      "|2024-03-15 02:56:34|  0.4| 0.4|17.7|0.1|    -999|        9.1|NULL|\n",
      "|2024-03-15 03:11:34|  0.6| 0.6|17.0|0.1|    -999|        9.1|NULL|\n",
      "|2024-03-15 03:26:34|  0.5| 0.5|19.8|0.1|    -999|        9.2|NULL|\n",
      "|2024-03-15 03:41:34|  0.5| 0.5|20.1|0.1|    -999|        9.0|NULL|\n",
      "|2024-03-15 03:56:34|  0.5| 0.5|22.5|0.1|    -999|        8.9|NULL|\n",
      "|2024-03-15 04:11:34|  0.6| 0.6|21.0|0.0|    -999|        8.7|NULL|\n",
      "|2024-03-15 04:26:34|  0.6| 0.6|21.0|0.1|    -999|        8.5|NULL|\n",
      "|2024-03-15 04:41:34|  0.5| 0.5|22.0|0.1|    -999|        8.6|NULL|\n",
      "|2024-03-15 04:56:34|  0.9| 1.1|21.7|0.1|    -999|        8.4|NULL|\n",
      "+-------------------+-----+----+----+---+--------+-----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF_LIST_AQ_SEP_S = [standardize_df(df) for df in DF_LIST_AQ_SEP]\n",
    "TOTAL_LIST_AQ_SEP_S = 0\n",
    "for i, df in enumerate(DF_LIST_AQ_SEP_S):\n",
    "    print(f\"DataFrame DF_LIST_AQ_SEP_S {i} columns: {df.columns} count: {df.count()}\")\n",
    "    TOTAL_LIST_AQ_SEP_S += df.count()\n",
    "print(f\"Total de filas en todos los DataFrames: {TOTAL_LIST_AQ_SEP_S}\")\n",
    "DF_LIST_AQ_SEP_S[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame DF_LIST_AQ_01_S 0 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 2841\n",
      "DataFrame DF_LIST_AQ_01_S 1 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 12962\n",
      "DataFrame DF_LIST_AQ_01_S 2 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 14216\n",
      "DataFrame DF_LIST_AQ_01_S 3 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 14879\n",
      "DataFrame DF_LIST_AQ_01_S 4 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 14376\n",
      "DataFrame DF_LIST_AQ_01_S 5 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 14168\n",
      "DataFrame DF_LIST_AQ_01_S 6 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 13346\n",
      "DataFrame DF_LIST_AQ_01_S 7 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 15215\n",
      "DataFrame DF_LIST_AQ_01_S 8 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 14458\n",
      "DataFrame DF_LIST_AQ_01_S 9 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 13936\n",
      "DataFrame DF_LIST_AQ_01_S 10 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 13489\n",
      "DataFrame DF_LIST_AQ_01_S 11 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 13678\n",
      "DataFrame DF_LIST_AQ_01_S 12 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 14640\n",
      "DataFrame DF_LIST_AQ_01_S 13 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 9709\n",
      "Total de filas en todos los DataFrames: 181913\n",
      "+----------------+-----+----+----+----+--------+-----------+----+\n",
      "|            time|pm2_5|pm10|  co|  o3|humidity|temperature| pm5|\n",
      "+----------------+-----+----+----+----+--------+-----------+----+\n",
      "|19/03/2024 22:08| -999|-999|-999|-999|  -999.0|     -998.0|-999|\n",
      "| 01/04/2024 9:33| -999|-999|-999|-999|  -999.0|     -999.0|-999|\n",
      "|08/04/2024 18:39|   44|  58|   2|-999|    60.3|       20.3|  29|\n",
      "|08/04/2024 18:41|   46|  56|   3|-999|    60.3|       20.3|  30|\n",
      "|08/04/2024 18:43|   46|  56|   4|-999|    60.3|       20.3|  30|\n",
      "|08/04/2024 18:45|   46|  61|   5|-999|    60.4|       20.3|  28|\n",
      "|08/04/2024 18:48|   45|  61|   6|-999|    60.5|       20.3|  27|\n",
      "|08/04/2024 18:50|   45|  61|   7|-999|    60.6|       20.3|  27|\n",
      "|08/04/2024 18:52|   46|  61|   8|-999|    60.7|       20.4|  28|\n",
      "|08/04/2024 18:54|   44|  53|   9|-999|    60.7|       20.4|  28|\n",
      "|08/04/2024 18:56|   44|  53|  10|-999|    60.7|       20.4|  28|\n",
      "|08/04/2024 19:00|   43|  52|  12|-999|     0.0|        0.0|  27|\n",
      "|08/04/2024 19:02|   40|  56|  13|-999|    60.8|       20.5|  26|\n",
      "|08/04/2024 19:09|   41|  56|   3|-999|    60.9|       20.5|  25|\n",
      "|08/04/2024 19:14|   41|  50|   5|-999|    60.9|       20.5|  26|\n",
      "|08/04/2024 19:16|   41|  50|   6|-999|    60.8|       20.5|  26|\n",
      "|08/04/2024 19:18|   41|  50|   7|-999|    60.2|       20.5|  26|\n",
      "|08/04/2024 19:20|   41|  50|   8|-999|    59.6|       20.5|  26|\n",
      "|08/04/2024 19:28|   33|  48|  12|-999|    58.8|       20.4|  22|\n",
      "|08/04/2024 19:30|   37|  52|  13|-999|    58.6|       20.4|  24|\n",
      "+----------------+-----+----+----+----+--------+-----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF_LIST_AQ_01_S = [standardize_df(df) for df in DF_LIST_AQ_01]\n",
    "TOTAL_LIST_AQ_01_S = 0\n",
    "for i, df in enumerate(DF_LIST_AQ_01_S):\n",
    "    print(f\"DataFrame DF_LIST_AQ_01_S {i} columns: {df.columns} count: {df.count()}\")\n",
    "    TOTAL_LIST_AQ_01_S += df.count()\n",
    "print(f\"Total de filas en todos los DataFrames: {TOTAL_LIST_AQ_01_S}\")\n",
    "DF_LIST_AQ_01_S[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame DF_LIST_AQ_02_S 0 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 2803\n",
      "DataFrame DF_LIST_AQ_02_S 1 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 6471\n",
      "DataFrame DF_LIST_AQ_02_S 2 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 3564\n",
      "DataFrame DF_LIST_AQ_02_S 3 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 14385\n",
      "DataFrame DF_LIST_AQ_02_S 4 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 13991\n",
      "DataFrame DF_LIST_AQ_02_S 5 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 13471\n",
      "DataFrame DF_LIST_AQ_02_S 6 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 14786\n",
      "DataFrame DF_LIST_AQ_02_S 7 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 14245\n",
      "DataFrame DF_LIST_AQ_02_S 8 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 14045\n",
      "DataFrame DF_LIST_AQ_02_S 9 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 13449\n",
      "DataFrame DF_LIST_AQ_02_S 10 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 13720\n",
      "DataFrame DF_LIST_AQ_02_S 11 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 14627\n",
      "DataFrame DF_LIST_AQ_02_S 12 columns: ['time', 'pm2_5', 'pm10', 'co', 'o3', 'humidity', 'temperature', 'pm5'] count: 8995\n",
      "Total de filas en todos los DataFrames: 148552\n",
      "+----------------+-----+----+----+----+--------+-----------+----+\n",
      "|            time|pm2_5|pm10|  co|  o3|humidity|temperature| pm5|\n",
      "+----------------+-----+----+----+----+--------+-----------+----+\n",
      "|19/03/2024 22:09| -999|-999|-999|-999|  -999.0|     -998.0|-999|\n",
      "| 01/04/2024 9:33| -999|-999|-999|-999|  -999.0|     -999.0|-999|\n",
      "|08/04/2024 20:18|   39|  45|   1|-999|    78.0|       19.0|  30|\n",
      "|08/04/2024 20:20|   42|  54|   2|-999|    78.1|       19.3|  28|\n",
      "|08/04/2024 20:46|   40|  47|   1|-999|    72.0|       20.3|  30|\n",
      "|08/04/2024 20:48|   45|  59|   2|-999|    71.1|       20.4|  30|\n",
      "|08/04/2024 20:53|   44|  56|   4|-999|    69.5|       20.5|  29|\n",
      "|08/04/2024 20:55|   41|  51|   5|-999|    69.1|       20.5|  27|\n",
      "|08/04/2024 20:57|   41|  52|   6|-999|    68.7|       20.6|  28|\n",
      "|08/04/2024 20:59|   40|  52|   7|-999|    68.4|       20.6|  27|\n",
      "|08/04/2024 21:05|   38|  48|  10|-999|    67.7|       20.8|  26|\n",
      "|08/04/2024 21:07|   38|  51|  11|-999|    67.5|       20.8|  27|\n",
      "|08/04/2024 21:11|   39|  50|  13|-999|    67.3|       20.9|  26|\n",
      "|08/04/2024 21:16|   36|  46|  15|-999|    66.9|       21.1|  26|\n",
      "|08/04/2024 21:20|   38|  45|  17|-999|    66.5|       21.2|  26|\n",
      "|08/04/2024 21:24|   38|  49|  19|-999|    66.1|       21.3|  26|\n",
      "|08/04/2024 21:26|   34|  44|  20|-999|    66.0|       21.3|  24|\n",
      "|08/04/2024 21:28|   36|  48|  21|-999|    65.8|       21.4|  24|\n",
      "|08/04/2024 21:30|   33|  39|  22|-999|    65.6|       21.4|  22|\n",
      "|08/04/2024 21:32|   35|  43|  23|-999|    65.4|       21.4|  23|\n",
      "+----------------+-----+----+----+----+--------+-----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF_LIST_AQ_02_S = [standardize_df(df) for df in DF_LIST_AQ_02]\n",
    "TOTAL_LIST_AQ_02_S = 0\n",
    "for i, df in enumerate(DF_LIST_AQ_02_S):\n",
    "    print(f\"DataFrame DF_LIST_AQ_02_S {i} columns: {df.columns} count: {df.count()}\")\n",
    "    TOTAL_LIST_AQ_02_S += df.count()\n",
    "print(f\"Total de filas en todos los DataFrames: {TOTAL_LIST_AQ_02_S}\")\n",
    "DF_LIST_AQ_02_S[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de filas en todos los DF_AQ_SEP: 32172\n",
      "+-------------------+-----+----+----+---+--------+-----------+----+\n",
      "|               time|pm2_5|pm10|  co| o3|humidity|temperature| pm5|\n",
      "+-------------------+-----+----+----+---+--------+-----------+----+\n",
      "|2024-03-15 00:11:34|  0.5| 0.5|25.3|0.1|  -999.0|       11.5|NULL|\n",
      "|2024-03-15 00:26:34|  0.7| 0.7|23.0|0.1|  -999.0|       11.6|NULL|\n",
      "|2024-03-15 00:41:34|  0.7| 0.7|22.2|0.1|  -999.0|       11.2|NULL|\n",
      "|2024-03-15 00:56:34|  0.6| 0.6|20.4|0.1|  -999.0|       11.1|NULL|\n",
      "|2024-03-15 01:11:34|  0.6| 0.6|19.6|0.1|  -999.0|       10.6|NULL|\n",
      "|2024-03-15 01:26:34|  0.5| 0.5|19.2|0.1|  -999.0|       10.6|NULL|\n",
      "|2024-03-15 01:41:34|  0.7| 0.7|19.5|0.1|  -999.0|       10.4|NULL|\n",
      "|2024-03-15 01:56:34|  0.6| 0.6|21.2|0.1|  -999.0|       10.5|NULL|\n",
      "|2024-03-15 02:11:34|  0.5| 0.5|20.3|0.1|  -999.0|       10.1|NULL|\n",
      "|2024-03-15 02:26:34|  0.6| 0.6|19.2|0.1|  -999.0|        9.9|NULL|\n",
      "|2024-03-15 02:41:34|  0.5| 0.5|19.7|0.1|  -999.0|        9.7|NULL|\n",
      "|2024-03-15 02:56:34|  0.4| 0.4|17.7|0.1|  -999.0|        9.1|NULL|\n",
      "|2024-03-15 03:11:34|  0.6| 0.6|17.0|0.1|  -999.0|        9.1|NULL|\n",
      "|2024-03-15 03:26:34|  0.5| 0.5|19.8|0.1|  -999.0|        9.2|NULL|\n",
      "|2024-03-15 03:41:34|  0.5| 0.5|20.1|0.1|  -999.0|        9.0|NULL|\n",
      "|2024-03-15 03:56:34|  0.5| 0.5|22.5|0.1|  -999.0|        8.9|NULL|\n",
      "|2024-03-15 04:11:34|  0.6| 0.6|21.0|0.0|  -999.0|        8.7|NULL|\n",
      "|2024-03-15 04:26:34|  0.6| 0.6|21.0|0.1|  -999.0|        8.5|NULL|\n",
      "|2024-03-15 04:41:34|  0.5| 0.5|22.0|0.1|  -999.0|        8.6|NULL|\n",
      "|2024-03-15 04:56:34|  0.9| 1.1|21.7|0.1|  -999.0|        8.4|NULL|\n",
      "+-------------------+-----+----+----+---+--------+-----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combinar todos los DataFrames en uno solo - DF_LIST_AQ_SEP_S\n",
    "DF_AQ_SEP = reduce(lambda df1, df2: df1.unionByName(df2), DF_LIST_AQ_SEP_S)\n",
    "print(f\"Total de filas en todos los DF_AQ_SEP: {DF_AQ_SEP.count()}\")\n",
    "DF_AQ_SEP.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de filas en todos los DF_AQ_1: 181913\n",
      "+----------------+-----+----+----+----+--------+-----------+----+\n",
      "|            time|pm2_5|pm10|  co|  o3|humidity|temperature| pm5|\n",
      "+----------------+-----+----+----+----+--------+-----------+----+\n",
      "|19/03/2024 22:08| -999|-999|-999|-999|  -999.0|     -998.0|-999|\n",
      "| 01/04/2024 9:33| -999|-999|-999|-999|  -999.0|     -999.0|-999|\n",
      "|08/04/2024 18:39|   44|  58|   2|-999|    60.3|       20.3|  29|\n",
      "|08/04/2024 18:41|   46|  56|   3|-999|    60.3|       20.3|  30|\n",
      "|08/04/2024 18:43|   46|  56|   4|-999|    60.3|       20.3|  30|\n",
      "|08/04/2024 18:45|   46|  61|   5|-999|    60.4|       20.3|  28|\n",
      "|08/04/2024 18:48|   45|  61|   6|-999|    60.5|       20.3|  27|\n",
      "|08/04/2024 18:50|   45|  61|   7|-999|    60.6|       20.3|  27|\n",
      "|08/04/2024 18:52|   46|  61|   8|-999|    60.7|       20.4|  28|\n",
      "|08/04/2024 18:54|   44|  53|   9|-999|    60.7|       20.4|  28|\n",
      "|08/04/2024 18:56|   44|  53|  10|-999|    60.7|       20.4|  28|\n",
      "|08/04/2024 19:00|   43|  52|  12|-999|     0.0|        0.0|  27|\n",
      "|08/04/2024 19:02|   40|  56|  13|-999|    60.8|       20.5|  26|\n",
      "|08/04/2024 19:09|   41|  56|   3|-999|    60.9|       20.5|  25|\n",
      "|08/04/2024 19:14|   41|  50|   5|-999|    60.9|       20.5|  26|\n",
      "|08/04/2024 19:16|   41|  50|   6|-999|    60.8|       20.5|  26|\n",
      "|08/04/2024 19:18|   41|  50|   7|-999|    60.2|       20.5|  26|\n",
      "|08/04/2024 19:20|   41|  50|   8|-999|    59.6|       20.5|  26|\n",
      "|08/04/2024 19:28|   33|  48|  12|-999|    58.8|       20.4|  22|\n",
      "|08/04/2024 19:30|   37|  52|  13|-999|    58.6|       20.4|  24|\n",
      "+----------------+-----+----+----+----+--------+-----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combinar todos los DataFrames en uno solo - DF_LIST_AQ_01_S\n",
    "DF_AQ_1 = reduce(lambda df1, df2: df1.unionByName(df2), DF_LIST_AQ_01_S)\n",
    "print(f\"Total de filas en todos los DF_AQ_1: {DF_AQ_1.count()}\")\n",
    "DF_AQ_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de filas en todos los DF_AQ_2: 148552\n",
      "+----------------+-----+----+----+----+--------+-----------+----+\n",
      "|            time|pm2_5|pm10|  co|  o3|humidity|temperature| pm5|\n",
      "+----------------+-----+----+----+----+--------+-----------+----+\n",
      "|19/03/2024 22:09| -999|-999|-999|-999|  -999.0|     -998.0|-999|\n",
      "| 01/04/2024 9:33| -999|-999|-999|-999|  -999.0|     -999.0|-999|\n",
      "|08/04/2024 20:18|   39|  45|   1|-999|    78.0|       19.0|  30|\n",
      "|08/04/2024 20:20|   42|  54|   2|-999|    78.1|       19.3|  28|\n",
      "|08/04/2024 20:46|   40|  47|   1|-999|    72.0|       20.3|  30|\n",
      "|08/04/2024 20:48|   45|  59|   2|-999|    71.1|       20.4|  30|\n",
      "|08/04/2024 20:53|   44|  56|   4|-999|    69.5|       20.5|  29|\n",
      "|08/04/2024 20:55|   41|  51|   5|-999|    69.1|       20.5|  27|\n",
      "|08/04/2024 20:57|   41|  52|   6|-999|    68.7|       20.6|  28|\n",
      "|08/04/2024 20:59|   40|  52|   7|-999|    68.4|       20.6|  27|\n",
      "|08/04/2024 21:05|   38|  48|  10|-999|    67.7|       20.8|  26|\n",
      "|08/04/2024 21:07|   38|  51|  11|-999|    67.5|       20.8|  27|\n",
      "|08/04/2024 21:11|   39|  50|  13|-999|    67.3|       20.9|  26|\n",
      "|08/04/2024 21:16|   36|  46|  15|-999|    66.9|       21.1|  26|\n",
      "|08/04/2024 21:20|   38|  45|  17|-999|    66.5|       21.2|  26|\n",
      "|08/04/2024 21:24|   38|  49|  19|-999|    66.1|       21.3|  26|\n",
      "|08/04/2024 21:26|   34|  44|  20|-999|    66.0|       21.3|  24|\n",
      "|08/04/2024 21:28|   36|  48|  21|-999|    65.8|       21.4|  24|\n",
      "|08/04/2024 21:30|   33|  39|  22|-999|    65.6|       21.4|  22|\n",
      "|08/04/2024 21:32|   35|  43|  23|-999|    65.4|       21.4|  23|\n",
      "+----------------+-----+----+----+----+--------+-----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combinar todos los DataFrames en uno solo - DF_LIST_AQ_02_S\n",
    "DF_AQ_2 = reduce(lambda df1, df2: df1.unionByName(df2), DF_LIST_AQ_02_S)\n",
    "print(f\"Total de filas en todos los DF_AQ_2: {DF_AQ_2.count()}\")\n",
    "DF_AQ_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DF_AQ_SEP -> Cantidad de filas completamente null: 0\n",
      " DF_AQ_1 -> Cantidad de filas completamente null: 0\n",
      " DF_AQ_2 -> Cantidad de filas completamente null: 0\n"
     ]
    }
   ],
   "source": [
    "# Crear una condición que sea True cuando todas las columnas sean null\n",
    "\n",
    "CONDITION = reduce(lambda acc, col: acc & F.col(col).isNull(), DF_AQ_2.columns, F.lit(True))\n",
    "\n",
    "# Filtrar y contar las filas que cumplen la condición\n",
    "DF_AQ_SEP_ALL_NULL = DF_AQ_SEP.filter(CONDITION).count()\n",
    "DF_AQ_1_ALL_NULL = DF_AQ_1.filter(CONDITION).count()\n",
    "DF_AQ_2_ALL_NULL = DF_AQ_2.filter(CONDITION).count()\n",
    "\n",
    "\n",
    "print(\" DF_AQ_SEP -> Cantidad de filas completamente null:\", DF_AQ_SEP_ALL_NULL)\n",
    "print(\" DF_AQ_1 -> Cantidad de filas completamente null:\", DF_AQ_1_ALL_NULL)\n",
    "print(\" DF_AQ_2 -> Cantidad de filas completamente null:\", DF_AQ_2_ALL_NULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_time_column(df):\n",
    "    \"\"\"\n",
    "    Procesa la columna 'time' de un DataFrame y crea una nueva columna 'time_format'\n",
    "    con el formato 'yyyy-MM-dd HH:mm:ss'.\n",
    "    \n",
    "    Considera varios formatos de fecha/hora:\n",
    "      - dd/MM/yyyy H:mm       (Ej: 19/03/2024 22:09 o 01/04/2024 9:33)\n",
    "      - yyyy-MM-dd H:mm:ss     (Ej: 2024-03-15 00:11:34)\n",
    "      - yyyy-MM-dd H:mm        (Ej: 2024-03-15 11:34)\n",
    "    \"\"\"\n",
    "    # Intenta convertir la columna \"time\" usando los diferentes formatos\n",
    "    df = df.withColumn(\n",
    "        \"time_ts\",\n",
    "        coalesce(\n",
    "            to_timestamp(col(\"time\"), \"dd/MM/yyyy H:mm\"),\n",
    "            to_timestamp(col(\"time\"), \"yyyy-MM-dd H:mm:ss\"),\n",
    "            to_timestamp(col(\"time\"), \"yyyy-MM-dd H:mm\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Formatea el timestamp al formato deseado y crea la nueva columna \"time_format\"\n",
    "    df = df.withColumn(\"time_format\", date_format(col(\"time_ts\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    \n",
    "    # Elimina la columna intermedia si ya no es necesaria\n",
    "    df = df.drop(\"time_ts\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se crea una funcion que estandariza las fechas y agrega una nueva columna\n",
    "DF_AQ_SEP = standardize_time_column(DF_AQ_SEP)\n",
    "DF_AQ_1   = standardize_time_column(DF_AQ_1)\n",
    "DF_AQ_2   = standardize_time_column(DF_AQ_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+----+----+---+--------+-----------+----+-------------------+\n",
      "|               time|pm2_5|pm10|  co| o3|humidity|temperature| pm5|        time_format|\n",
      "+-------------------+-----+----+----+---+--------+-----------+----+-------------------+\n",
      "|2024-03-15 00:11:34|  0.5| 0.5|25.3|0.1|  -999.0|       11.5|NULL|2024-03-15 00:11:34|\n",
      "|2024-03-15 00:26:34|  0.7| 0.7|23.0|0.1|  -999.0|       11.6|NULL|2024-03-15 00:26:34|\n",
      "|2024-03-15 00:41:34|  0.7| 0.7|22.2|0.1|  -999.0|       11.2|NULL|2024-03-15 00:41:34|\n",
      "|2024-03-15 00:56:34|  0.6| 0.6|20.4|0.1|  -999.0|       11.1|NULL|2024-03-15 00:56:34|\n",
      "|2024-03-15 01:11:34|  0.6| 0.6|19.6|0.1|  -999.0|       10.6|NULL|2024-03-15 01:11:34|\n",
      "|2024-03-15 01:26:34|  0.5| 0.5|19.2|0.1|  -999.0|       10.6|NULL|2024-03-15 01:26:34|\n",
      "|2024-03-15 01:41:34|  0.7| 0.7|19.5|0.1|  -999.0|       10.4|NULL|2024-03-15 01:41:34|\n",
      "|2024-03-15 01:56:34|  0.6| 0.6|21.2|0.1|  -999.0|       10.5|NULL|2024-03-15 01:56:34|\n",
      "|2024-03-15 02:11:34|  0.5| 0.5|20.3|0.1|  -999.0|       10.1|NULL|2024-03-15 02:11:34|\n",
      "|2024-03-15 02:26:34|  0.6| 0.6|19.2|0.1|  -999.0|        9.9|NULL|2024-03-15 02:26:34|\n",
      "|2024-03-15 02:41:34|  0.5| 0.5|19.7|0.1|  -999.0|        9.7|NULL|2024-03-15 02:41:34|\n",
      "|2024-03-15 02:56:34|  0.4| 0.4|17.7|0.1|  -999.0|        9.1|NULL|2024-03-15 02:56:34|\n",
      "|2024-03-15 03:11:34|  0.6| 0.6|17.0|0.1|  -999.0|        9.1|NULL|2024-03-15 03:11:34|\n",
      "|2024-03-15 03:26:34|  0.5| 0.5|19.8|0.1|  -999.0|        9.2|NULL|2024-03-15 03:26:34|\n",
      "|2024-03-15 03:41:34|  0.5| 0.5|20.1|0.1|  -999.0|        9.0|NULL|2024-03-15 03:41:34|\n",
      "|2024-03-15 03:56:34|  0.5| 0.5|22.5|0.1|  -999.0|        8.9|NULL|2024-03-15 03:56:34|\n",
      "|2024-03-15 04:11:34|  0.6| 0.6|21.0|0.0|  -999.0|        8.7|NULL|2024-03-15 04:11:34|\n",
      "|2024-03-15 04:26:34|  0.6| 0.6|21.0|0.1|  -999.0|        8.5|NULL|2024-03-15 04:26:34|\n",
      "|2024-03-15 04:41:34|  0.5| 0.5|22.0|0.1|  -999.0|        8.6|NULL|2024-03-15 04:41:34|\n",
      "|2024-03-15 04:56:34|  0.9| 1.1|21.7|0.1|  -999.0|        8.4|NULL|2024-03-15 04:56:34|\n",
      "+-------------------+-----+----+----+---+--------+-----------+----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF_AQ_SEP.show()\n",
    "#DF_AQ_1.show()\n",
    "#DF_AQ_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+---+--------+-----------+---+-----------+\n",
      "|time|pm2_5|pm10| co| o3|humidity|temperature|pm5|time_format|\n",
      "+----+-----+----+---+---+--------+-----------+---+-----------+\n",
      "+----+-----+----+---+---+--------+-----------+---+-----------+\n",
      "\n",
      "DF_AQ_SEP - Número de filas con time_format null: 0\n"
     ]
    }
   ],
   "source": [
    "# Filtrar las filas donde time_format es null\n",
    "df_null_time = DF_AQ_SEP.filter(col(\"time_format\").isNull())\n",
    "# Mostrar las filas filtradas\n",
    "df_null_time.show()\n",
    "# Opcional: contar el número de filas con time_format null\n",
    "null_count = DF_AQ_SEP.filter(col(\"time_format\").isNull()).count()\n",
    "print(\"DF_AQ_SEP - Número de filas con time_format null:\", null_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+---+--------+-----------+---+-----------+\n",
      "|time|pm2_5|pm10| co| o3|humidity|temperature|pm5|time_format|\n",
      "+----+-----+----+---+---+--------+-----------+---+-----------+\n",
      "+----+-----+----+---+---+--------+-----------+---+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1523:=======================================>              (11 + 4) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_AQ_1 - Número de filas con time_format null: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filtrar las filas donde time_format es null\n",
    "df_null_time = DF_AQ_1.filter(col(\"time_format\").isNull())\n",
    "# Mostrar las filas filtradas\n",
    "df_null_time.show()\n",
    "# Opcional: contar el número de filas con time_format null\n",
    "null_count = DF_AQ_1.filter(col(\"time_format\").isNull()).count()\n",
    "print(\"DF_AQ_1 - Número de filas con time_format null:\", null_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+---+--------+-----------+---+-----------+\n",
      "|time|pm2_5|pm10| co| o3|humidity|temperature|pm5|time_format|\n",
      "+----+-----+----+---+---+--------+-----------+---+-----------+\n",
      "+----+-----+----+---+---+--------+-----------+---+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1529:======================================>               (10 + 4) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_AQ_2 - Número de filas con time_format null: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filtrar las filas donde time_format es null\n",
    "df_null_time = DF_AQ_2.filter(col(\"time_format\").isNull())\n",
    "# Mostrar las filas filtradas\n",
    "df_null_time.show()\n",
    "# Opcional: contar el número de filas con time_format null\n",
    "null_count = DF_AQ_2.filter(col(\"time_format\").isNull()).count()\n",
    "print(\"DF_AQ_2 - Número de filas con time_format null:\", null_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#despues de verificar que la columna time_format no tiene valores nulos se procede a eliminar la columna time\n",
    "DF_AQ_SEP = DF_AQ_SEP.drop(\"time\")\n",
    "DF_AQ_SEP = DF_AQ_SEP.drop(\"pm5\")\n",
    "DF_AQ_1 = DF_AQ_1.drop(\"time\")\n",
    "DF_AQ_2 = DF_AQ_2.drop(\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_AQ_SEP = DF_AQ_SEP.withColumn(\"pm2_5\",\n",
    "    when(col(\"pm2_5\") <= -990, None).otherwise(col(\"pm2_5\"))\n",
    ")\n",
    "DF_AQ_SEP = DF_AQ_SEP.withColumn(\"pm10\",\n",
    "    when(col(\"pm10\") <= -990, None).otherwise(col(\"pm10\"))\n",
    ")\n",
    "DF_AQ_SEP = DF_AQ_SEP.withColumn(\"co\",\n",
    "    when(col(\"co\") <= -990, None).otherwise(col(\"co\"))\n",
    ")\n",
    "DF_AQ_SEP = DF_AQ_SEP.withColumn(\"o3\",\n",
    "    when(col(\"o3\") <= -990, None).otherwise(col(\"o3\"))\n",
    ")\n",
    "DF_AQ_SEP = DF_AQ_SEP.withColumn(\"humidity\",\n",
    "    when(col(\"humidity\") <= -990, None).otherwise(col(\"humidity\"))\n",
    ")\n",
    "DF_AQ_SEP = DF_AQ_SEP.withColumn(\"temperature\",\n",
    "    when(col(\"temperature\") <= -990, None).otherwise(col(\"temperature\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_AQ_1 = DF_AQ_1.withColumn(\"pm2_5\",\n",
    "    when(col(\"pm2_5\") <= -990, None).otherwise(col(\"pm2_5\"))\n",
    ")\n",
    "DF_AQ_1 = DF_AQ_1.withColumn(\"pm10\",\n",
    "    when(col(\"pm10\") <= -990, None).otherwise(col(\"pm10\"))\n",
    ")\n",
    "DF_AQ_1 = DF_AQ_1.withColumn(\"co\",\n",
    "    when(col(\"co\") <= -990, None).otherwise(col(\"co\"))\n",
    ")\n",
    "DF_AQ_1 = DF_AQ_1.withColumn(\"o3\",\n",
    "    when(col(\"o3\") <= -990, None).otherwise(col(\"o3\"))\n",
    ")\n",
    "DF_AQ_1 = DF_AQ_1.withColumn(\"humidity\",\n",
    "    when(col(\"humidity\") <= -990, None).otherwise(col(\"humidity\"))\n",
    ")\n",
    "DF_AQ_1 = DF_AQ_1.withColumn(\"temperature\",\n",
    "    when(col(\"temperature\") <= -990, None).otherwise(col(\"temperature\"))\n",
    ")\n",
    "DF_AQ_1 = DF_AQ_1.withColumn(\"pm5\",\n",
    "    when(col(\"pm5\") <= -990, None).otherwise(col(\"pm5\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_AQ_2 = DF_AQ_2.withColumn(\"pm2_5\",\n",
    "    when(col(\"pm2_5\") <= -990, None).otherwise(col(\"pm2_5\"))\n",
    ")\n",
    "DF_AQ_2 = DF_AQ_2.withColumn(\"pm10\",\n",
    "    when(col(\"pm10\") <= -990, None).otherwise(col(\"pm10\"))\n",
    ")\n",
    "DF_AQ_2 = DF_AQ_2.withColumn(\"co\",\n",
    "    when(col(\"co\") <= -990, None).otherwise(col(\"co\"))\n",
    ")\n",
    "DF_AQ_2 = DF_AQ_2.withColumn(\"o3\",\n",
    "    when(col(\"o3\") <= -990, None).otherwise(col(\"o3\"))\n",
    ")\n",
    "DF_AQ_2 = DF_AQ_2.withColumn(\"humidity\",\n",
    "    when(col(\"humidity\") <= -990, None).otherwise(col(\"humidity\"))\n",
    ")\n",
    "DF_AQ_2 = DF_AQ_2.withColumn(\"temperature\",\n",
    "    when(col(\"temperature\") <= -990, None).otherwise(col(\"temperature\"))\n",
    ")\n",
    "DF_AQ_2 = DF_AQ_2.withColumn(\"pm5\",\n",
    "    when(col(\"pm5\") <= -990, None).otherwise(col(\"pm5\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# FILTRAR FILAS CON VALORES NULOS EN COLUMNAS DIFERENTES AL TIME\n",
    "cols = [\"pm2_5\", \"pm10\", \"co\", \"o3\", \"humidity\", \"temperature\",\"pm5\"]\n",
    "# Construir la condición: cada columna debe ser null\n",
    "condition = reduce(lambda acc, c: acc & col(c).isNull(), cols, lit(True))\n",
    "# Filtrar el DataFrame\n",
    "DF_AQ_SEP_null_rows = DF_AQ_SEP.filter(condition)\n",
    "DF_AQ_2_null_rows = DF_AQ_2.filter(condition)\n",
    "DF_AQ_1_null_rows = DF_AQ_1.filter(condition)\n",
    "# Mostrar el resultado\n",
    "print(DF_AQ_SEP_null_rows.count())\n",
    "print(DF_AQ_1_null_rows.count())\n",
    "print(DF_AQ_2_null_rows.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AHORA SE ELIMINAN LAS FILAS CON VALORES NULOS\n",
    "# Lista de columnas a verificar\n",
    "cols = [\"pm2_5\", \"pm10\", \"co\", \"o3\", \"humidity\", \"temperature\",\"pm5\"]\n",
    "\n",
    "# Construir la condición que identifica filas donde todas las columnas son null\n",
    "condition_all_null = reduce(lambda acc, c: acc & col(c).isNull(), cols, lit(True))\n",
    "\n",
    "# Filtrar el DataFrame eliminando aquellas filas que cumplan la condición\n",
    "DF_AQ_SEP = DF_AQ_SEP.filter(~condition_all_null)\n",
    "DF_AQ_1 = DF_AQ_1.filter(~condition_all_null)\n",
    "DF_AQ_2 = DF_AQ_2.filter(~condition_all_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales\n",
    "engine = None\n",
    "\n",
    "def connect_to_db():\n",
    "    global engine\n",
    "    user = 'root'\n",
    "    password = 'rootpassword'\n",
    "    host = '127.0.0.1'\n",
    "    port = 3309\n",
    "    db = 'AIRQUALITY'\n",
    "\n",
    "    connection_string = f\"mysql+pymysql://{user}:{password}@{host}:{port}/{db}\"\n",
    "    engine = create_engine(connection_string)\n",
    "    print(\"✅ Conexión a MySQL creada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tables_if_not_exist():\n",
    "    with engine.connect() as conn:\n",
    "        # Tabla DF_AQ_SEP\n",
    "        conn.execute(text(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS DF_AQ_SEP (\n",
    "                pm2_5 DOUBLE,\n",
    "                pm10 DOUBLE,\n",
    "                co DOUBLE,\n",
    "                o3 DOUBLE,\n",
    "                humidity DOUBLE,\n",
    "                temperature DOUBLE,\n",
    "                time_format VARCHAR(50)\n",
    "            )\n",
    "        \"\"\"))\n",
    "\n",
    "        # Tabla DF_AQ_1\n",
    "        conn.execute(text(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS DF_AQ_1 (\n",
    "                pm2_5 INT,\n",
    "                pm10 INT,\n",
    "                co INT,\n",
    "                o3 INT,\n",
    "                humidity DOUBLE,\n",
    "                temperature DOUBLE,\n",
    "                pm5 INT,\n",
    "                time_format VARCHAR(50)\n",
    "            )\n",
    "        \"\"\"))\n",
    "\n",
    "        # Tabla DF_AQ_2\n",
    "        conn.execute(text(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS DF_AQ_2 (\n",
    "                pm2_5 INT,\n",
    "                pm10 INT,\n",
    "                co INT,\n",
    "                o3 INT,\n",
    "                humidity DOUBLE,\n",
    "                temperature DOUBLE,\n",
    "                pm5 INT,\n",
    "                time_format VARCHAR(50)\n",
    "            )\n",
    "        \"\"\"))\n",
    "\n",
    "        print(\"✅ Tablas verificadas y creadas si era necesario.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_dataframe(df, table_name):\n",
    "    if engine is None:\n",
    "        raise Exception(\"Primero debes establecer la conexión con `connect_to_db()`.\")\n",
    "\n",
    "    try:\n",
    "        df.to_sql(name=table_name, con=engine, if_exists='append', index=False)\n",
    "        print(f\"✅ Datos insertados en la tabla {table_name}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al insertar datos en la tabla {table_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conexión a MySQL creada.\n",
      "✅ Tablas verificadas y creadas si era necesario.\n",
      "✅ Datos insertados en la tabla DF_AQ_SEP.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Datos insertados en la tabla DF_AQ_1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Datos insertados en la tabla DF_AQ_2.\n"
     ]
    }
   ],
   "source": [
    "connect_to_db()\n",
    "create_tables_if_not_exist()\n",
    "\n",
    "# Insertar los dataframes limpiando primero cada tabla\n",
    "insert_dataframe(DF_AQ_SEP.toPandas(), \"DF_AQ_SEP\")\n",
    "insert_dataframe(DF_AQ_1.toPandas(), \"DF_AQ_1\")\n",
    "insert_dataframe(DF_AQ_2.toPandas(), \"DF_AQ_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+---+--------+-----------+-------------------+\n",
      "|pm2_5|pm10|  co| o3|humidity|temperature|        time_format|\n",
      "+-----+----+----+---+--------+-----------+-------------------+\n",
      "|  0.5| 0.5|25.3|0.1|    NULL|       11.5|2024-03-15 00:11:34|\n",
      "|  0.7| 0.7|23.0|0.1|    NULL|       11.6|2024-03-15 00:26:34|\n",
      "|  0.7| 0.7|22.2|0.1|    NULL|       11.2|2024-03-15 00:41:34|\n",
      "|  0.6| 0.6|20.4|0.1|    NULL|       11.1|2024-03-15 00:56:34|\n",
      "|  0.6| 0.6|19.6|0.1|    NULL|       10.6|2024-03-15 01:11:34|\n",
      "|  0.5| 0.5|19.2|0.1|    NULL|       10.6|2024-03-15 01:26:34|\n",
      "|  0.7| 0.7|19.5|0.1|    NULL|       10.4|2024-03-15 01:41:34|\n",
      "|  0.6| 0.6|21.2|0.1|    NULL|       10.5|2024-03-15 01:56:34|\n",
      "|  0.5| 0.5|20.3|0.1|    NULL|       10.1|2024-03-15 02:11:34|\n",
      "|  0.6| 0.6|19.2|0.1|    NULL|        9.9|2024-03-15 02:26:34|\n",
      "|  0.5| 0.5|19.7|0.1|    NULL|        9.7|2024-03-15 02:41:34|\n",
      "|  0.4| 0.4|17.7|0.1|    NULL|        9.1|2024-03-15 02:56:34|\n",
      "|  0.6| 0.6|17.0|0.1|    NULL|        9.1|2024-03-15 03:11:34|\n",
      "|  0.5| 0.5|19.8|0.1|    NULL|        9.2|2024-03-15 03:26:34|\n",
      "|  0.5| 0.5|20.1|0.1|    NULL|        9.0|2024-03-15 03:41:34|\n",
      "|  0.5| 0.5|22.5|0.1|    NULL|        8.9|2024-03-15 03:56:34|\n",
      "|  0.6| 0.6|21.0|0.0|    NULL|        8.7|2024-03-15 04:11:34|\n",
      "|  0.6| 0.6|21.0|0.1|    NULL|        8.5|2024-03-15 04:26:34|\n",
      "|  0.5| 0.5|22.0|0.1|    NULL|        8.6|2024-03-15 04:41:34|\n",
      "|  0.9| 1.1|21.7|0.1|    NULL|        8.4|2024-03-15 04:56:34|\n",
      "+-----+----+----+---+--------+-----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF_AQ_SEP.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "air2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
